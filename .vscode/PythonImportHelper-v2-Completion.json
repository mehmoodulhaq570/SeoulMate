[
    {
        "label": "FastAPI",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "Query",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "CORSMiddleware",
        "importPath": "fastapi.middleware.cors",
        "description": "fastapi.middleware.cors",
        "isExtraImport": true,
        "detail": "fastapi.middleware.cors",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pickle",
        "description": "pickle",
        "detail": "pickle",
        "documentation": {}
    },
    {
        "label": "faiss",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "faiss",
        "description": "faiss",
        "detail": "faiss",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "SentenceTransformer",
        "importPath": "sentence_transformers",
        "description": "sentence_transformers",
        "isExtraImport": true,
        "detail": "sentence_transformers",
        "documentation": {}
    },
    {
        "label": "CrossEncoder",
        "importPath": "sentence_transformers",
        "description": "sentence_transformers",
        "isExtraImport": true,
        "detail": "sentence_transformers",
        "documentation": {}
    },
    {
        "label": "SentenceTransformer",
        "importPath": "sentence_transformers",
        "description": "sentence_transformers",
        "isExtraImport": true,
        "detail": "sentence_transformers",
        "documentation": {}
    },
    {
        "label": "process",
        "importPath": "rapidfuzz",
        "description": "rapidfuzz",
        "isExtraImport": true,
        "detail": "rapidfuzz",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "rapidfuzz",
        "description": "rapidfuzz",
        "isExtraImport": true,
        "detail": "rapidfuzz",
        "documentation": {}
    },
    {
        "label": "lru_cache",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "BM25Okapi",
        "importPath": "rank_bm25",
        "description": "rank_bm25",
        "isExtraImport": true,
        "detail": "rank_bm25",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "aiohttp",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "aiohttp",
        "description": "aiohttp",
        "detail": "aiohttp",
        "documentation": {}
    },
    {
        "label": "asyncio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "asyncio",
        "description": "asyncio",
        "detail": "asyncio",
        "documentation": {}
    },
    {
        "label": "aiofiles",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "aiofiles",
        "description": "aiofiles",
        "detail": "aiofiles",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm.asyncio",
        "description": "tqdm.asyncio",
        "isExtraImport": true,
        "detail": "tqdm.asyncio",
        "documentation": {}
    },
    {
        "label": "urlparse",
        "importPath": "urllib.parse",
        "description": "urllib.parse",
        "isExtraImport": true,
        "detail": "urllib.parse",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "csv",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "csv",
        "description": "csv",
        "detail": "csv",
        "documentation": {}
    },
    {
        "label": "ThreadPoolExecutor",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "as_completed",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "html",
        "importPath": "lxml",
        "description": "lxml",
        "isExtraImport": true,
        "detail": "lxml",
        "documentation": {}
    },
    {
        "label": "async_playwright",
        "importPath": "playwright.async_api",
        "description": "playwright.async_api",
        "isExtraImport": true,
        "detail": "playwright.async_api",
        "documentation": {}
    },
    {
        "label": "Route",
        "importPath": "playwright.async_api",
        "description": "playwright.async_api",
        "isExtraImport": true,
        "detail": "playwright.async_api",
        "documentation": {}
    },
    {
        "label": "TimeoutError",
        "importPath": "playwright.async_api",
        "description": "playwright.async_api",
        "isExtraImport": true,
        "detail": "playwright.async_api",
        "documentation": {}
    },
    {
        "label": "async_playwright",
        "importPath": "playwright.async_api",
        "description": "playwright.async_api",
        "isExtraImport": true,
        "detail": "playwright.async_api",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "sync_playwright",
        "importPath": "playwright.sync_api",
        "description": "playwright.sync_api",
        "isExtraImport": true,
        "detail": "playwright.sync_api",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "webdriver",
        "importPath": "selenium",
        "description": "selenium",
        "isExtraImport": true,
        "detail": "selenium",
        "documentation": {}
    },
    {
        "label": "By",
        "importPath": "selenium.webdriver.common.by",
        "description": "selenium.webdriver.common.by",
        "isExtraImport": true,
        "detail": "selenium.webdriver.common.by",
        "documentation": {}
    },
    {
        "label": "Options",
        "importPath": "selenium.webdriver.chrome.options",
        "description": "selenium.webdriver.chrome.options",
        "isExtraImport": true,
        "detail": "selenium.webdriver.chrome.options",
        "documentation": {}
    },
    {
        "label": "fuzzy_match_title",
        "kind": 2,
        "importPath": "backend.app",
        "description": "backend.app",
        "peekOfCode": "def fuzzy_match_title(user_input: str, threshold=70):\n    \"\"\"Handle typos and near matches using fuzzy logic.\"\"\"\n    match, score, _ = process.extractOne(user_input, titles, scorer=fuzz.WRatio)\n    if score >= threshold:\n        return match, score\n    return None, score\n@lru_cache(maxsize=128)\ndef cached_encode(text: str):\n    \"\"\"Cached embedding generation for speed.\"\"\"\n    emb = model.encode([text], convert_to_numpy=True)",
        "detail": "backend.app",
        "documentation": {}
    },
    {
        "label": "cached_encode",
        "kind": 2,
        "importPath": "backend.app",
        "description": "backend.app",
        "peekOfCode": "def cached_encode(text: str):\n    \"\"\"Cached embedding generation for speed.\"\"\"\n    emb = model.encode([text], convert_to_numpy=True)\n    faiss.normalize_L2(emb)\n    return emb\n# ======================================================\n# STAGE 4 — HYBRID RECOMMENDATION PIPELINE\n# ======================================================\ndef recommend(title: str, top_n=5, alpha=0.7):\n    \"\"\"",
        "detail": "backend.app",
        "documentation": {}
    },
    {
        "label": "recommend",
        "kind": 2,
        "importPath": "backend.app",
        "description": "backend.app",
        "peekOfCode": "def recommend(title: str, top_n=5, alpha=0.7):\n    \"\"\"\n    Stage-based pipeline:\n    1. Resolve user input (fuzzy match or free-text)\n    2. Semantic search (FAISS)\n    3. Lexical search (BM25)\n    4. Hybrid combination\n    5. Optional reranking (Cross-Encoder)\n    \"\"\"\n    # ---- Stage 4.1: Title resolution ----",
        "detail": "backend.app",
        "documentation": {}
    },
    {
        "label": "root",
        "kind": 2,
        "importPath": "backend.app",
        "description": "backend.app",
        "peekOfCode": "def root():\n    return {\"message\": \"Hybrid Kdrama Recommendation API v3.1 is running\"}\n@app.get(\"/recommend\")\ndef get_recommendations(\n    title: str = Query(..., description=\"Kdrama title or user query\"),\n    top_n: int = 5\n):\n    \"\"\"Main recommendation endpoint.\"\"\"\n    return recommend(title, top_n)\n# ======================================================",
        "detail": "backend.app",
        "documentation": {}
    },
    {
        "label": "get_recommendations",
        "kind": 2,
        "importPath": "backend.app",
        "description": "backend.app",
        "peekOfCode": "def get_recommendations(\n    title: str = Query(..., description=\"Kdrama title or user query\"),\n    top_n: int = 5\n):\n    \"\"\"Main recommendation endpoint.\"\"\"\n    return recommend(title, top_n)\n# ======================================================\n# STAGE 6 — RUN LOCALLY\n# ======================================================\nif __name__ == \"__main__\":",
        "detail": "backend.app",
        "documentation": {}
    },
    {
        "label": "MODEL_NAME",
        "kind": 5,
        "importPath": "backend.app",
        "description": "backend.app",
        "peekOfCode": "MODEL_NAME = \"paraphrase-multilingual-mpnet-base-v2\"\nCROSS_ENCODER_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\nMODEL_DIR = r\"D:\\Projects\\Kdrama-recommendation\\model_training\\models\"\nINDEX_DIR = r\"D:\\Projects\\Kdrama-recommendation\\model_training\\faiss_index\"\n# ======================================================\n# FASTAPI SETUP\n# ======================================================\napp = FastAPI(title=\"Kdrama Hybrid Recommendation API\", version=\"3.1\")\napp.add_middleware(\n    CORSMiddleware,",
        "detail": "backend.app",
        "documentation": {}
    },
    {
        "label": "CROSS_ENCODER_MODEL",
        "kind": 5,
        "importPath": "backend.app",
        "description": "backend.app",
        "peekOfCode": "CROSS_ENCODER_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\nMODEL_DIR = r\"D:\\Projects\\Kdrama-recommendation\\model_training\\models\"\nINDEX_DIR = r\"D:\\Projects\\Kdrama-recommendation\\model_training\\faiss_index\"\n# ======================================================\n# FASTAPI SETUP\n# ======================================================\napp = FastAPI(title=\"Kdrama Hybrid Recommendation API\", version=\"3.1\")\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],",
        "detail": "backend.app",
        "documentation": {}
    },
    {
        "label": "MODEL_DIR",
        "kind": 5,
        "importPath": "backend.app",
        "description": "backend.app",
        "peekOfCode": "MODEL_DIR = r\"D:\\Projects\\Kdrama-recommendation\\model_training\\models\"\nINDEX_DIR = r\"D:\\Projects\\Kdrama-recommendation\\model_training\\faiss_index\"\n# ======================================================\n# FASTAPI SETUP\n# ======================================================\napp = FastAPI(title=\"Kdrama Hybrid Recommendation API\", version=\"3.1\")\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,",
        "detail": "backend.app",
        "documentation": {}
    },
    {
        "label": "INDEX_DIR",
        "kind": 5,
        "importPath": "backend.app",
        "description": "backend.app",
        "peekOfCode": "INDEX_DIR = r\"D:\\Projects\\Kdrama-recommendation\\model_training\\faiss_index\"\n# ======================================================\n# FASTAPI SETUP\n# ======================================================\napp = FastAPI(title=\"Kdrama Hybrid Recommendation API\", version=\"3.1\")\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],",
        "detail": "backend.app",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "backend.app",
        "description": "backend.app",
        "peekOfCode": "app = FastAPI(title=\"Kdrama Hybrid Recommendation API\", version=\"3.1\")\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n# ======================================================\n# STAGE 1 — LOAD MODELS & INDEXES",
        "detail": "backend.app",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "backend.app",
        "description": "backend.app",
        "peekOfCode": "model = SentenceTransformer(MODEL_NAME, cache_folder=MODEL_DIR)\nindex = faiss.read_index(os.path.join(INDEX_DIR, \"index.faiss\"))\nwith open(os.path.join(INDEX_DIR, \"meta.pkl\"), \"rb\") as f:\n    metadata = pickle.load(f)\ntitles = [m[\"Title\"] for m in metadata]\ncorpus = [\n    f\"{m.get('Title', '')} {m.get('Genre', '')} {m.get('Description', '')} {m.get('Cast', '')}\"\n    for m in metadata\n]\nbm25 = BM25Okapi([doc.split() for doc in corpus])",
        "detail": "backend.app",
        "documentation": {}
    },
    {
        "label": "index",
        "kind": 5,
        "importPath": "backend.app",
        "description": "backend.app",
        "peekOfCode": "index = faiss.read_index(os.path.join(INDEX_DIR, \"index.faiss\"))\nwith open(os.path.join(INDEX_DIR, \"meta.pkl\"), \"rb\") as f:\n    metadata = pickle.load(f)\ntitles = [m[\"Title\"] for m in metadata]\ncorpus = [\n    f\"{m.get('Title', '')} {m.get('Genre', '')} {m.get('Description', '')} {m.get('Cast', '')}\"\n    for m in metadata\n]\nbm25 = BM25Okapi([doc.split() for doc in corpus])\nprint(f\"Loaded {len(metadata)} dramas successfully.\")",
        "detail": "backend.app",
        "documentation": {}
    },
    {
        "label": "titles",
        "kind": 5,
        "importPath": "backend.app",
        "description": "backend.app",
        "peekOfCode": "titles = [m[\"Title\"] for m in metadata]\ncorpus = [\n    f\"{m.get('Title', '')} {m.get('Genre', '')} {m.get('Description', '')} {m.get('Cast', '')}\"\n    for m in metadata\n]\nbm25 = BM25Okapi([doc.split() for doc in corpus])\nprint(f\"Loaded {len(metadata)} dramas successfully.\")\n# ======================================================\n# STAGE 2 — LOAD OPTIONAL RERANKER\n# ======================================================",
        "detail": "backend.app",
        "documentation": {}
    },
    {
        "label": "corpus",
        "kind": 5,
        "importPath": "backend.app",
        "description": "backend.app",
        "peekOfCode": "corpus = [\n    f\"{m.get('Title', '')} {m.get('Genre', '')} {m.get('Description', '')} {m.get('Cast', '')}\"\n    for m in metadata\n]\nbm25 = BM25Okapi([doc.split() for doc in corpus])\nprint(f\"Loaded {len(metadata)} dramas successfully.\")\n# ======================================================\n# STAGE 2 — LOAD OPTIONAL RERANKER\n# ======================================================\ntry:",
        "detail": "backend.app",
        "documentation": {}
    },
    {
        "label": "bm25",
        "kind": 5,
        "importPath": "backend.app",
        "description": "backend.app",
        "peekOfCode": "bm25 = BM25Okapi([doc.split() for doc in corpus])\nprint(f\"Loaded {len(metadata)} dramas successfully.\")\n# ======================================================\n# STAGE 2 — LOAD OPTIONAL RERANKER\n# ======================================================\ntry:\n    print(\"Stage 2: Loading cross-encoder reranker...\")\n    reranker = CrossEncoder(CROSS_ENCODER_MODEL)\n    use_reranker = True\n    print(\"Cross-encoder reranker loaded successfully.\")",
        "detail": "backend.app",
        "documentation": {}
    },
    {
        "label": "extract_notes_from_title",
        "kind": 2,
        "importPath": "data_scrapper.dataset.clean_kdrama_dataset",
        "description": "data_scrapper.dataset.clean_kdrama_dataset",
        "peekOfCode": "def extract_notes_from_title(title):\n    \"\"\"Extract text in () and [] from title and return cleaned title and notes\"\"\"\n    if not isinstance(title, str):\n        return title, \"_\"\n    # Find all content in parentheses and brackets\n    notes_list = []\n    # Extract content in parentheses ()\n    parentheses = re.findall(r'\\([^)]+\\)', title)\n    notes_list.extend(parentheses)\n    # Extract content in brackets []",
        "detail": "data_scrapper.dataset.clean_kdrama_dataset",
        "documentation": {}
    },
    {
        "label": "clean_release_years_value",
        "kind": 2,
        "importPath": "data_scrapper.dataset.clean_kdrama_dataset",
        "description": "data_scrapper.dataset.clean_kdrama_dataset",
        "peekOfCode": "def clean_release_years_value(val):\n    \"\"\"Keep only the second year if multiple years exist, replace missing/- with _\"\"\"\n    if not isinstance(val, str):\n        return \"_\"\n    val = val.strip()\n    # Replace - with _\n    if val == \"-\" or val == \"\" or val == \"nan\":\n        return \"_\"\n    # If there are multiple years separated by comma, keep the second one (after comma)\n    if ',' in val:",
        "detail": "data_scrapper.dataset.clean_kdrama_dataset",
        "documentation": {}
    },
    {
        "label": "input_file",
        "kind": 5,
        "importPath": "data_scrapper.dataset.clean_kdrama_dataset",
        "description": "data_scrapper.dataset.clean_kdrama_dataset",
        "peekOfCode": "input_file = os.path.join(os.path.dirname(__file__), \"kdrama_dataset.csv\")\ndf = pd.read_csv(input_file, dtype=str)\nprint(f\"Loaded {len(df)} rows\")\nprint(f\"Columns: {list(df.columns)}\")\n# Add 'notes' column if it doesn't exist\nif 'notes' not in df.columns:\n    df['notes'] = \"_\"\n    print(\"Added 'notes' column\")\n# Function to extract content from parentheses and brackets\ndef extract_notes_from_title(title):",
        "detail": "data_scrapper.dataset.clean_kdrama_dataset",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "data_scrapper.dataset.clean_kdrama_dataset",
        "description": "data_scrapper.dataset.clean_kdrama_dataset",
        "peekOfCode": "df = pd.read_csv(input_file, dtype=str)\nprint(f\"Loaded {len(df)} rows\")\nprint(f\"Columns: {list(df.columns)}\")\n# Add 'notes' column if it doesn't exist\nif 'notes' not in df.columns:\n    df['notes'] = \"_\"\n    print(\"Added 'notes' column\")\n# Function to extract content from parentheses and brackets\ndef extract_notes_from_title(title):\n    \"\"\"Extract text in () and [] from title and return cleaned title and notes\"\"\"",
        "detail": "data_scrapper.dataset.clean_kdrama_dataset",
        "documentation": {}
    },
    {
        "label": "cleaned_titles",
        "kind": 5,
        "importPath": "data_scrapper.dataset.clean_kdrama_dataset",
        "description": "data_scrapper.dataset.clean_kdrama_dataset",
        "peekOfCode": "cleaned_titles = []\nextracted_notes = []\nfor idx, title in enumerate(df['title']):\n    cleaned_title, notes = extract_notes_from_title(title)\n    cleaned_titles.append(cleaned_title)\n    extracted_notes.append(notes)\ndf['title'] = cleaned_titles\n# Update notes column (append to existing notes if any)\nfor idx, note in enumerate(extracted_notes):\n    existing_note = df.loc[idx, 'notes']",
        "detail": "data_scrapper.dataset.clean_kdrama_dataset",
        "documentation": {}
    },
    {
        "label": "extracted_notes",
        "kind": 5,
        "importPath": "data_scrapper.dataset.clean_kdrama_dataset",
        "description": "data_scrapper.dataset.clean_kdrama_dataset",
        "peekOfCode": "extracted_notes = []\nfor idx, title in enumerate(df['title']):\n    cleaned_title, notes = extract_notes_from_title(title)\n    cleaned_titles.append(cleaned_title)\n    extracted_notes.append(notes)\ndf['title'] = cleaned_titles\n# Update notes column (append to existing notes if any)\nfor idx, note in enumerate(extracted_notes):\n    existing_note = df.loc[idx, 'notes']\n    if pd.isna(existing_note) or existing_note == \"_\" or existing_note == \"\":",
        "detail": "data_scrapper.dataset.clean_kdrama_dataset",
        "documentation": {}
    },
    {
        "label": "df['title']",
        "kind": 5,
        "importPath": "data_scrapper.dataset.clean_kdrama_dataset",
        "description": "data_scrapper.dataset.clean_kdrama_dataset",
        "peekOfCode": "df['title'] = cleaned_titles\n# Update notes column (append to existing notes if any)\nfor idx, note in enumerate(extracted_notes):\n    existing_note = df.loc[idx, 'notes']\n    if pd.isna(existing_note) or existing_note == \"_\" or existing_note == \"\":\n        df.loc[idx, 'notes'] = note\n    elif note != \"_\":\n        df.loc[idx, 'notes'] = f\"{existing_note}, {note}\"\nprint(f\"Extracted notes from {sum(1 for n in extracted_notes if n != '_')} titles\")\n# Clean release_years column",
        "detail": "data_scrapper.dataset.clean_kdrama_dataset",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "data_scrapper.dataset.clean_kdrama_dataset",
        "description": "data_scrapper.dataset.clean_kdrama_dataset",
        "peekOfCode": "df = df.fillna(\"_\")\ndf = df.replace(\"\", \"_\")\ndf = df.replace(\"-\", \"_\")\ndf = df.replace(\"nan\", \"_\")\n# Save the cleaned dataset\noutput_csv = os.path.join(os.path.dirname(__file__), \"kdrama_dataset_cleaned.csv\")\noutput_excel = os.path.join(os.path.dirname(__file__), \"kdrama_dataset_cleaned.xlsx\")\ndf.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\ndf.to_excel(output_excel, index=False)\nprint(f\"\\n✓ Cleaned dataset saved to: {output_csv}\")",
        "detail": "data_scrapper.dataset.clean_kdrama_dataset",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "data_scrapper.dataset.clean_kdrama_dataset",
        "description": "data_scrapper.dataset.clean_kdrama_dataset",
        "peekOfCode": "df = df.replace(\"\", \"_\")\ndf = df.replace(\"-\", \"_\")\ndf = df.replace(\"nan\", \"_\")\n# Save the cleaned dataset\noutput_csv = os.path.join(os.path.dirname(__file__), \"kdrama_dataset_cleaned.csv\")\noutput_excel = os.path.join(os.path.dirname(__file__), \"kdrama_dataset_cleaned.xlsx\")\ndf.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\ndf.to_excel(output_excel, index=False)\nprint(f\"\\n✓ Cleaned dataset saved to: {output_csv}\")\nprint(f\"✓ Cleaned dataset also saved to: {output_excel}\")",
        "detail": "data_scrapper.dataset.clean_kdrama_dataset",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "data_scrapper.dataset.clean_kdrama_dataset",
        "description": "data_scrapper.dataset.clean_kdrama_dataset",
        "peekOfCode": "df = df.replace(\"-\", \"_\")\ndf = df.replace(\"nan\", \"_\")\n# Save the cleaned dataset\noutput_csv = os.path.join(os.path.dirname(__file__), \"kdrama_dataset_cleaned.csv\")\noutput_excel = os.path.join(os.path.dirname(__file__), \"kdrama_dataset_cleaned.xlsx\")\ndf.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\ndf.to_excel(output_excel, index=False)\nprint(f\"\\n✓ Cleaned dataset saved to: {output_csv}\")\nprint(f\"✓ Cleaned dataset also saved to: {output_excel}\")\nprint(f\"\\nTotal rows: {len(df)}\")",
        "detail": "data_scrapper.dataset.clean_kdrama_dataset",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "data_scrapper.dataset.clean_kdrama_dataset",
        "description": "data_scrapper.dataset.clean_kdrama_dataset",
        "peekOfCode": "df = df.replace(\"nan\", \"_\")\n# Save the cleaned dataset\noutput_csv = os.path.join(os.path.dirname(__file__), \"kdrama_dataset_cleaned.csv\")\noutput_excel = os.path.join(os.path.dirname(__file__), \"kdrama_dataset_cleaned.xlsx\")\ndf.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\ndf.to_excel(output_excel, index=False)\nprint(f\"\\n✓ Cleaned dataset saved to: {output_csv}\")\nprint(f\"✓ Cleaned dataset also saved to: {output_excel}\")\nprint(f\"\\nTotal rows: {len(df)}\")\nprint(f\"Columns: {list(df.columns)}\")",
        "detail": "data_scrapper.dataset.clean_kdrama_dataset",
        "documentation": {}
    },
    {
        "label": "output_csv",
        "kind": 5,
        "importPath": "data_scrapper.dataset.clean_kdrama_dataset",
        "description": "data_scrapper.dataset.clean_kdrama_dataset",
        "peekOfCode": "output_csv = os.path.join(os.path.dirname(__file__), \"kdrama_dataset_cleaned.csv\")\noutput_excel = os.path.join(os.path.dirname(__file__), \"kdrama_dataset_cleaned.xlsx\")\ndf.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\ndf.to_excel(output_excel, index=False)\nprint(f\"\\n✓ Cleaned dataset saved to: {output_csv}\")\nprint(f\"✓ Cleaned dataset also saved to: {output_excel}\")\nprint(f\"\\nTotal rows: {len(df)}\")\nprint(f\"Columns: {list(df.columns)}\")\nprint(\"\\n✓ Processing complete!\")",
        "detail": "data_scrapper.dataset.clean_kdrama_dataset",
        "documentation": {}
    },
    {
        "label": "output_excel",
        "kind": 5,
        "importPath": "data_scrapper.dataset.clean_kdrama_dataset",
        "description": "data_scrapper.dataset.clean_kdrama_dataset",
        "peekOfCode": "output_excel = os.path.join(os.path.dirname(__file__), \"kdrama_dataset_cleaned.xlsx\")\ndf.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\ndf.to_excel(output_excel, index=False)\nprint(f\"\\n✓ Cleaned dataset saved to: {output_csv}\")\nprint(f\"✓ Cleaned dataset also saved to: {output_excel}\")\nprint(f\"\\nTotal rows: {len(df)}\")\nprint(f\"Columns: {list(df.columns)}\")\nprint(\"\\n✓ Processing complete!\")",
        "detail": "data_scrapper.dataset.clean_kdrama_dataset",
        "documentation": {}
    },
    {
        "label": "extract_years",
        "kind": 2,
        "importPath": "data_scrapper.dataset.merger",
        "description": "data_scrapper.dataset.merger",
        "peekOfCode": "def extract_years(date_str):\n    if not isinstance(date_str, str):\n        return \"_\"\n    # Find all 4-digit years\n    years = re.findall(r\"(19|20)\\d{2}\", date_str)\n    if years:\n        # Return the highest year as string\n        return max(years, key=int)\n    # Try to extract year from date formats like 10/12/2019\n    match = re.search(r\"(19|20)\\d{2}\", date_str)",
        "detail": "data_scrapper.dataset.merger",
        "documentation": {}
    },
    {
        "label": "clean_release_years",
        "kind": 2,
        "importPath": "data_scrapper.dataset.merger",
        "description": "data_scrapper.dataset.merger",
        "peekOfCode": "def clean_release_years(val, date_published):\n    # Only fill if value is '_'\n    if not isinstance(val, str) or val.strip() == \"_\":\n        return extract_years(date_published)\n    # If multiple years, keep only the highest\n    years = re.findall(r\"(19|20)\\d{2}\", val)\n    if years:\n        return max(years, key=int)\n    return val\nif \"release_years\" in combined_df.columns and \"date_published\" in combined_df.columns:",
        "detail": "data_scrapper.dataset.merger",
        "documentation": {}
    },
    {
        "label": "input_file",
        "kind": 5,
        "importPath": "data_scrapper.dataset.merger",
        "description": "data_scrapper.dataset.merger",
        "peekOfCode": "input_file = os.path.join(os.path.dirname(__file__), \"combined_kdrama_dataset.csv\")\ncombined_df = pd.read_csv(input_file, dtype=str)\ndef extract_years(date_str):\n    if not isinstance(date_str, str):\n        return \"_\"\n    # Find all 4-digit years\n    years = re.findall(r\"(19|20)\\d{2}\", date_str)\n    if years:\n        # Return the highest year as string\n        return max(years, key=int)",
        "detail": "data_scrapper.dataset.merger",
        "documentation": {}
    },
    {
        "label": "combined_df",
        "kind": 5,
        "importPath": "data_scrapper.dataset.merger",
        "description": "data_scrapper.dataset.merger",
        "peekOfCode": "combined_df = pd.read_csv(input_file, dtype=str)\ndef extract_years(date_str):\n    if not isinstance(date_str, str):\n        return \"_\"\n    # Find all 4-digit years\n    years = re.findall(r\"(19|20)\\d{2}\", date_str)\n    if years:\n        # Return the highest year as string\n        return max(years, key=int)\n    # Try to extract year from date formats like 10/12/2019",
        "detail": "data_scrapper.dataset.merger",
        "documentation": {}
    },
    {
        "label": "output_csv",
        "kind": 5,
        "importPath": "data_scrapper.dataset.merger",
        "description": "data_scrapper.dataset.merger",
        "peekOfCode": "output_csv = os.path.join(\n    os.path.dirname(__file__), \"combined_kdrama_dataset_cleaned.csv\"\n)\noutput_excel = os.path.join(\n    os.path.dirname(__file__), \"combined_kdrama_dataset_cleaned.xlsx\"\n)\ncombined_df.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\ncombined_df.to_excel(output_excel, index=False)\nprint(f\"Cleaned dataset saved to: {output_csv}\")\nprint(f\"Cleaned dataset also saved to: {output_excel}\")",
        "detail": "data_scrapper.dataset.merger",
        "documentation": {}
    },
    {
        "label": "output_excel",
        "kind": 5,
        "importPath": "data_scrapper.dataset.merger",
        "description": "data_scrapper.dataset.merger",
        "peekOfCode": "output_excel = os.path.join(\n    os.path.dirname(__file__), \"combined_kdrama_dataset_cleaned.xlsx\"\n)\ncombined_df.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\ncombined_df.to_excel(output_excel, index=False)\nprint(f\"Cleaned dataset saved to: {output_csv}\")\nprint(f\"Cleaned dataset also saved to: {output_excel}\")\nprint(f\"Total rows: {len(combined_df)}\")\nprint(f\"Columns: {list(combined_df.columns)}\")",
        "detail": "data_scrapper.dataset.merger",
        "documentation": {}
    },
    {
        "label": "sanitize_filename",
        "kind": 2,
        "importPath": "data_scrapper.DramaList_Scrapper.dramaImage",
        "description": "data_scrapper.DramaList_Scrapper.dramaImage",
        "peekOfCode": "def sanitize_filename(name):\n    \"\"\"Remove invalid characters for safe file naming.\"\"\"\n    return re.sub(r'[\\\\/*?:\"<>|]', \"_\", str(name)).strip()\nUSER_AGENTS = [\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n    \"(KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\",\n    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 13_5_2) AppleWebKit/605.1.15 \"\n    \"(KHTML, like Gecko) Version/17.0 Safari/605.1.15\",\n    \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:126.0) Gecko/20100101 Firefox/126.0\",\n    \"Mozilla/5.0 (Linux; Android 14; SM-S918B) AppleWebKit/537.36 \"",
        "detail": "data_scrapper.DramaList_Scrapper.dramaImage",
        "documentation": {}
    },
    {
        "label": "download_images_from_csv",
        "kind": 2,
        "importPath": "data_scrapper.DramaList_Scrapper.dramaImage",
        "description": "data_scrapper.DramaList_Scrapper.dramaImage",
        "peekOfCode": "def download_images_from_csv(csv_path, output_folder, concurrency=100):\n    \"\"\"Download only missing images from a CSV/Excel file.\"\"\"\n    # Read data\n    if csv_path.lower().endswith(('.xlsx', '.xls')):\n        df = pd.read_excel(csv_path)\n    else:\n        df = pd.read_csv(csv_path, sep=None, engine=\"python\")\n    if \"title\" not in df.columns or \"image\" not in df.columns:\n        raise ValueError(\"The file must contain 'title' and 'image' columns.\")\n    os.makedirs(output_folder, exist_ok=True)",
        "detail": "data_scrapper.DramaList_Scrapper.dramaImage",
        "documentation": {}
    },
    {
        "label": "USER_AGENTS",
        "kind": 5,
        "importPath": "data_scrapper.DramaList_Scrapper.dramaImage",
        "description": "data_scrapper.DramaList_Scrapper.dramaImage",
        "peekOfCode": "USER_AGENTS = [\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n    \"(KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\",\n    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 13_5_2) AppleWebKit/605.1.15 \"\n    \"(KHTML, like Gecko) Version/17.0 Safari/605.1.15\",\n    \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:126.0) Gecko/20100101 Firefox/126.0\",\n    \"Mozilla/5.0 (Linux; Android 14; SM-S918B) AppleWebKit/537.36 \"\n    \"(KHTML, like Gecko) Chrome/125.0.0.0 Mobile Safari/537.36\",\n    \"Mozilla/5.0 (iPhone; CPU iPhone OS 17_3 like Mac OS X) \"\n    \"AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1\",",
        "detail": "data_scrapper.DramaList_Scrapper.dramaImage",
        "documentation": {}
    },
    {
        "label": "extract_mydramalist_data",
        "kind": 2,
        "importPath": "data_scrapper.DramaList_Scrapper.scrapper",
        "description": "data_scrapper.DramaList_Scrapper.scrapper",
        "peekOfCode": "def extract_mydramalist_data(file_path):\n    \"\"\"Ultra-fast extractor using lxml (no BeautifulSoup).\"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n            text = f.read()\n    except Exception:\n        return None\n    try:\n        doc = html.fromstring(text)\n    except Exception:",
        "detail": "data_scrapper.DramaList_Scrapper.scrapper",
        "documentation": {}
    },
    {
        "label": "process_folder",
        "kind": 2,
        "importPath": "data_scrapper.DramaList_Scrapper.scrapper",
        "description": "data_scrapper.DramaList_Scrapper.scrapper",
        "peekOfCode": "def process_folder(input_path, output_csv=\"output_fast.csv\", max_workers=None, skip_existing=True):\n    \"\"\"Process all HTML files using multithreading and lxml for maximum speed.\"\"\"\n    if not os.path.exists(input_path):\n        print(f\"Path not found: {input_path}\")\n        return\n    # Collect HTML files\n    if os.path.isdir(input_path):\n        files = [os.path.join(input_path, f) for f in os.listdir(input_path)\n                 if f.lower().endswith(\".html\")]\n    else:",
        "detail": "data_scrapper.DramaList_Scrapper.scrapper",
        "documentation": {}
    },
    {
        "label": "TVSERIES_RE",
        "kind": 5,
        "importPath": "data_scrapper.DramaList_Scrapper.scrapper",
        "description": "data_scrapper.DramaList_Scrapper.scrapper",
        "peekOfCode": "TVSERIES_RE = re.compile('TVSeries')\nDESC_CLASS_RE = re.compile(r'(show-synopsis|show-synopsis__text|show-details-item__content)')\ndef extract_mydramalist_data(file_path):\n    \"\"\"Ultra-fast extractor using lxml (no BeautifulSoup).\"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n            text = f.read()\n    except Exception:\n        return None\n    try:",
        "detail": "data_scrapper.DramaList_Scrapper.scrapper",
        "documentation": {}
    },
    {
        "label": "DESC_CLASS_RE",
        "kind": 5,
        "importPath": "data_scrapper.DramaList_Scrapper.scrapper",
        "description": "data_scrapper.DramaList_Scrapper.scrapper",
        "peekOfCode": "DESC_CLASS_RE = re.compile(r'(show-synopsis|show-synopsis__text|show-details-item__content)')\ndef extract_mydramalist_data(file_path):\n    \"\"\"Ultra-fast extractor using lxml (no BeautifulSoup).\"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n            text = f.read()\n    except Exception:\n        return None\n    try:\n        doc = html.fromstring(text)",
        "detail": "data_scrapper.DramaList_Scrapper.scrapper",
        "documentation": {}
    },
    {
        "label": "remove_emojis",
        "kind": 2,
        "importPath": "data_scrapper.DramaList_Scrapper.scrapper_2",
        "description": "data_scrapper.DramaList_Scrapper.scrapper_2",
        "peekOfCode": "def remove_emojis(text):\n    return re.sub(r\"[\\U00010000-\\U0010ffff]\", \"\", text)\n# -------------------------------------------\n# Download single page\n# -------------------------------------------\nasync def download_page(i, url, context, output_dir, semaphore):\n    url = url.strip()\n    if (\n        not url\n        or url.lower() in [\"n/a\", \"none\", \"null\"]",
        "detail": "data_scrapper.DramaList_Scrapper.scrapper_2",
        "documentation": {}
    },
    {
        "label": "USER_AGENTS",
        "kind": 5,
        "importPath": "data_scrapper.DramaList_Scrapper.scrapper_2",
        "description": "data_scrapper.DramaList_Scrapper.scrapper_2",
        "peekOfCode": "USER_AGENTS = [\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36\",\n    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15\",\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:127.0) Gecko/20100101 Firefox/127.0\",\n    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36\",\n]\n# -------------------------------------------\n# Block unnecessary resources\n# -------------------------------------------\nasync def block_images_and_fonts(route: Route):",
        "detail": "data_scrapper.DramaList_Scrapper.scrapper_2",
        "documentation": {}
    },
    {
        "label": "get_rating_value",
        "kind": 2,
        "importPath": "data_scrapper.DramaList_Scrapper.test",
        "description": "data_scrapper.DramaList_Scrapper.test",
        "peekOfCode": "def get_rating_value(label):\n    tag = soup.select_one(f'li.list-item b.inline:-soup-contains(\"{label}\")')\n    if tag:\n        return tag.parent.get_text().replace(label + \":\", \"\").strip()\n    return None\nratings = {}\n# Score: extract only numeric value\nscore_text = get_rating_value(\"Score\")\nif score_text:\n    match = re.search(r\"[\\d\\.]+\", score_text)",
        "detail": "data_scrapper.DramaList_Scrapper.test",
        "documentation": {}
    },
    {
        "label": "html_path",
        "kind": 5,
        "importPath": "data_scrapper.DramaList_Scrapper.test",
        "description": "data_scrapper.DramaList_Scrapper.test",
        "peekOfCode": "html_path = r\"D:\\Projects\\SeoulMate\\data_scrapper\\DramaList_Scrapper\\dramas_html\\799280-my-secret-vampire.html\"\nif not os.path.exists(html_path):\n    raise FileNotFoundError(f\"HTML file not found: {html_path}\")\n# ---------------------------\n# Read HTML file\n# ---------------------------\nwith open(html_path, \"r\", encoding=\"utf-8\") as f:\n    html_content = f.read()\nsoup = BeautifulSoup(html_content, \"html.parser\")\n# ---------------------------",
        "detail": "data_scrapper.DramaList_Scrapper.test",
        "documentation": {}
    },
    {
        "label": "soup",
        "kind": 5,
        "importPath": "data_scrapper.DramaList_Scrapper.test",
        "description": "data_scrapper.DramaList_Scrapper.test",
        "peekOfCode": "soup = BeautifulSoup(html_content, \"html.parser\")\n# ---------------------------\n# 1. Titles\n# ---------------------------\ntitles = {}\nnative_tag = soup.select_one(\"li.list-item b.inline + a\")\ntitles[\"native\"] = native_tag.text.strip() if native_tag else None\nenglish_tag = soup.select_one(\"li.active\")\ntitles[\"english\"] = english_tag.text.strip() if english_tag else None\naka_span = soup.select_one(\"li.list-item b.inline + span.mdl-aka-titles\")",
        "detail": "data_scrapper.DramaList_Scrapper.test",
        "documentation": {}
    },
    {
        "label": "titles",
        "kind": 5,
        "importPath": "data_scrapper.DramaList_Scrapper.test",
        "description": "data_scrapper.DramaList_Scrapper.test",
        "peekOfCode": "titles = {}\nnative_tag = soup.select_one(\"li.list-item b.inline + a\")\ntitles[\"native\"] = native_tag.text.strip() if native_tag else None\nenglish_tag = soup.select_one(\"li.active\")\ntitles[\"english\"] = english_tag.text.strip() if english_tag else None\naka_span = soup.select_one(\"li.list-item b.inline + span.mdl-aka-titles\")\ntitles[\"also_known_as\"] = (\n    [x.strip() for x in aka_span.text.split(\",\")] if aka_span else []\n)\n# ---------------------------",
        "detail": "data_scrapper.DramaList_Scrapper.test",
        "documentation": {}
    },
    {
        "label": "native_tag",
        "kind": 5,
        "importPath": "data_scrapper.DramaList_Scrapper.test",
        "description": "data_scrapper.DramaList_Scrapper.test",
        "peekOfCode": "native_tag = soup.select_one(\"li.list-item b.inline + a\")\ntitles[\"native\"] = native_tag.text.strip() if native_tag else None\nenglish_tag = soup.select_one(\"li.active\")\ntitles[\"english\"] = english_tag.text.strip() if english_tag else None\naka_span = soup.select_one(\"li.list-item b.inline + span.mdl-aka-titles\")\ntitles[\"also_known_as\"] = (\n    [x.strip() for x in aka_span.text.split(\",\")] if aka_span else []\n)\n# ---------------------------\n# 2. Related content",
        "detail": "data_scrapper.DramaList_Scrapper.test",
        "documentation": {}
    },
    {
        "label": "titles[\"native\"]",
        "kind": 5,
        "importPath": "data_scrapper.DramaList_Scrapper.test",
        "description": "data_scrapper.DramaList_Scrapper.test",
        "peekOfCode": "titles[\"native\"] = native_tag.text.strip() if native_tag else None\nenglish_tag = soup.select_one(\"li.active\")\ntitles[\"english\"] = english_tag.text.strip() if english_tag else None\naka_span = soup.select_one(\"li.list-item b.inline + span.mdl-aka-titles\")\ntitles[\"also_known_as\"] = (\n    [x.strip() for x in aka_span.text.split(\",\")] if aka_span else []\n)\n# ---------------------------\n# 2. Related content\n# ---------------------------",
        "detail": "data_scrapper.DramaList_Scrapper.test",
        "documentation": {}
    },
    {
        "label": "english_tag",
        "kind": 5,
        "importPath": "data_scrapper.DramaList_Scrapper.test",
        "description": "data_scrapper.DramaList_Scrapper.test",
        "peekOfCode": "english_tag = soup.select_one(\"li.active\")\ntitles[\"english\"] = english_tag.text.strip() if english_tag else None\naka_span = soup.select_one(\"li.list-item b.inline + span.mdl-aka-titles\")\ntitles[\"also_known_as\"] = (\n    [x.strip() for x in aka_span.text.split(\",\")] if aka_span else []\n)\n# ---------------------------\n# 2. Related content\n# ---------------------------\nrelated = []",
        "detail": "data_scrapper.DramaList_Scrapper.test",
        "documentation": {}
    },
    {
        "label": "titles[\"english\"]",
        "kind": 5,
        "importPath": "data_scrapper.DramaList_Scrapper.test",
        "description": "data_scrapper.DramaList_Scrapper.test",
        "peekOfCode": "titles[\"english\"] = english_tag.text.strip() if english_tag else None\naka_span = soup.select_one(\"li.list-item b.inline + span.mdl-aka-titles\")\ntitles[\"also_known_as\"] = (\n    [x.strip() for x in aka_span.text.split(\",\")] if aka_span else []\n)\n# ---------------------------\n# 2. Related content\n# ---------------------------\nrelated = []\nfor rel in soup.select(\"li.related-content div.title a.text-primary\"):",
        "detail": "data_scrapper.DramaList_Scrapper.test",
        "documentation": {}
    },
    {
        "label": "aka_span",
        "kind": 5,
        "importPath": "data_scrapper.DramaList_Scrapper.test",
        "description": "data_scrapper.DramaList_Scrapper.test",
        "peekOfCode": "aka_span = soup.select_one(\"li.list-item b.inline + span.mdl-aka-titles\")\ntitles[\"also_known_as\"] = (\n    [x.strip() for x in aka_span.text.split(\",\")] if aka_span else []\n)\n# ---------------------------\n# 2. Related content\n# ---------------------------\nrelated = []\nfor rel in soup.select(\"li.related-content div.title a.text-primary\"):\n    related.append({\"title\": rel.text.strip(), \"url\": rel.get(\"href\", \"\")})",
        "detail": "data_scrapper.DramaList_Scrapper.test",
        "documentation": {}
    },
    {
        "label": "titles[\"also_known_as\"]",
        "kind": 5,
        "importPath": "data_scrapper.DramaList_Scrapper.test",
        "description": "data_scrapper.DramaList_Scrapper.test",
        "peekOfCode": "titles[\"also_known_as\"] = (\n    [x.strip() for x in aka_span.text.split(\",\")] if aka_span else []\n)\n# ---------------------------\n# 2. Related content\n# ---------------------------\nrelated = []\nfor rel in soup.select(\"li.related-content div.title a.text-primary\"):\n    related.append({\"title\": rel.text.strip(), \"url\": rel.get(\"href\", \"\")})\n# ---------------------------",
        "detail": "data_scrapper.DramaList_Scrapper.test",
        "documentation": {}
    },
    {
        "label": "related",
        "kind": 5,
        "importPath": "data_scrapper.DramaList_Scrapper.test",
        "description": "data_scrapper.DramaList_Scrapper.test",
        "peekOfCode": "related = []\nfor rel in soup.select(\"li.related-content div.title a.text-primary\"):\n    related.append({\"title\": rel.text.strip(), \"url\": rel.get(\"href\", \"\")})\n# ---------------------------\n# 3. Directors\n# ---------------------------\ndirectors = [\n    a.text.strip()\n    for a in soup.select('li.list-item b.inline:-soup-contains(\"Director\") ~ a')\n]",
        "detail": "data_scrapper.DramaList_Scrapper.test",
        "documentation": {}
    },
    {
        "label": "directors",
        "kind": 5,
        "importPath": "data_scrapper.DramaList_Scrapper.test",
        "description": "data_scrapper.DramaList_Scrapper.test",
        "peekOfCode": "directors = [\n    a.text.strip()\n    for a in soup.select('li.list-item b.inline:-soup-contains(\"Director\") ~ a')\n]\n# ---------------------------\n# 4. Screenwriters\n# ---------------------------\nscreenwriters = [\n    a.text.strip()\n    for a in soup.select('li.list-item b.inline:-soup-contains(\"Screenwriter\") ~ a')",
        "detail": "data_scrapper.DramaList_Scrapper.test",
        "documentation": {}
    },
    {
        "label": "screenwriters",
        "kind": 5,
        "importPath": "data_scrapper.DramaList_Scrapper.test",
        "description": "data_scrapper.DramaList_Scrapper.test",
        "peekOfCode": "screenwriters = [\n    a.text.strip()\n    for a in soup.select('li.list-item b.inline:-soup-contains(\"Screenwriter\") ~ a')\n]\n# ---------------------------\n# 5. Genres\n# ---------------------------\ngenres = [a.text.strip() for a in soup.select(\"li.show-genres a.text-primary\")]\n# ---------------------------\n# 6. Tags",
        "detail": "data_scrapper.DramaList_Scrapper.test",
        "documentation": {}
    },
    {
        "label": "genres",
        "kind": 5,
        "importPath": "data_scrapper.DramaList_Scrapper.test",
        "description": "data_scrapper.DramaList_Scrapper.test",
        "peekOfCode": "genres = [a.text.strip() for a in soup.select(\"li.show-genres a.text-primary\")]\n# ---------------------------\n# 6. Tags\n# ---------------------------\ntags = [a.text.strip() for a in soup.select(\"li.show-tags span a.text-primary\")]\n# ---------------------------\n# 7. Country, type, episodes, aired info\n# ---------------------------\ninfo = {}\nfor li in soup.select(\"ul.list.hidden-md-up li\"):",
        "detail": "data_scrapper.DramaList_Scrapper.test",
        "documentation": {}
    },
    {
        "label": "tags",
        "kind": 5,
        "importPath": "data_scrapper.DramaList_Scrapper.test",
        "description": "data_scrapper.DramaList_Scrapper.test",
        "peekOfCode": "tags = [a.text.strip() for a in soup.select(\"li.show-tags span a.text-primary\")]\n# ---------------------------\n# 7. Country, type, episodes, aired info\n# ---------------------------\ninfo = {}\nfor li in soup.select(\"ul.list.hidden-md-up li\"):\n    key_tag = li.select_one(\"b.inline\")\n    if key_tag:\n        key = key_tag.text.strip(\":\")\n        val = li.text.replace(key + \":\", \"\").strip()",
        "detail": "data_scrapper.DramaList_Scrapper.test",
        "documentation": {}
    },
    {
        "label": "info",
        "kind": 5,
        "importPath": "data_scrapper.DramaList_Scrapper.test",
        "description": "data_scrapper.DramaList_Scrapper.test",
        "peekOfCode": "info = {}\nfor li in soup.select(\"ul.list.hidden-md-up li\"):\n    key_tag = li.select_one(\"b.inline\")\n    if key_tag:\n        key = key_tag.text.strip(\":\")\n        val = li.text.replace(key + \":\", \"\").strip()\n        info[key] = val\n# ---------------------------\n# 8. Ratings & Popularity\n# ---------------------------",
        "detail": "data_scrapper.DramaList_Scrapper.test",
        "documentation": {}
    },
    {
        "label": "ratings",
        "kind": 5,
        "importPath": "data_scrapper.DramaList_Scrapper.test",
        "description": "data_scrapper.DramaList_Scrapper.test",
        "peekOfCode": "ratings = {}\n# Score: extract only numeric value\nscore_text = get_rating_value(\"Score\")\nif score_text:\n    match = re.search(r\"[\\d\\.]+\", score_text)\n    ratings[\"score\"] = float(match.group()) if match else None\nelse:\n    ratings[\"score\"] = None\nratings[\"ranked\"] = get_rating_value(\"Ranked\")\nratings[\"popularity\"] = get_rating_value(\"Popularity\")",
        "detail": "data_scrapper.DramaList_Scrapper.test",
        "documentation": {}
    },
    {
        "label": "score_text",
        "kind": 5,
        "importPath": "data_scrapper.DramaList_Scrapper.test",
        "description": "data_scrapper.DramaList_Scrapper.test",
        "peekOfCode": "score_text = get_rating_value(\"Score\")\nif score_text:\n    match = re.search(r\"[\\d\\.]+\", score_text)\n    ratings[\"score\"] = float(match.group()) if match else None\nelse:\n    ratings[\"score\"] = None\nratings[\"ranked\"] = get_rating_value(\"Ranked\")\nratings[\"popularity\"] = get_rating_value(\"Popularity\")\nratings[\"content_rating\"] = get_rating_value(\"Content Rating\")\n# ---------------------------",
        "detail": "data_scrapper.DramaList_Scrapper.test",
        "documentation": {}
    },
    {
        "label": "ratings[\"ranked\"]",
        "kind": 5,
        "importPath": "data_scrapper.DramaList_Scrapper.test",
        "description": "data_scrapper.DramaList_Scrapper.test",
        "peekOfCode": "ratings[\"ranked\"] = get_rating_value(\"Ranked\")\nratings[\"popularity\"] = get_rating_value(\"Popularity\")\nratings[\"content_rating\"] = get_rating_value(\"Content Rating\")\n# ---------------------------\n# 9. Cast\n# ---------------------------\ncast_list = []\nfor li in soup.select(\"ul.credits li.list-item\"):\n    actor_tag = li.select_one('b[itempropx=\"name\"]')\n    actor = actor_tag.text.strip() if actor_tag else \"\"",
        "detail": "data_scrapper.DramaList_Scrapper.test",
        "documentation": {}
    },
    {
        "label": "ratings[\"popularity\"]",
        "kind": 5,
        "importPath": "data_scrapper.DramaList_Scrapper.test",
        "description": "data_scrapper.DramaList_Scrapper.test",
        "peekOfCode": "ratings[\"popularity\"] = get_rating_value(\"Popularity\")\nratings[\"content_rating\"] = get_rating_value(\"Content Rating\")\n# ---------------------------\n# 9. Cast\n# ---------------------------\ncast_list = []\nfor li in soup.select(\"ul.credits li.list-item\"):\n    actor_tag = li.select_one('b[itempropx=\"name\"]')\n    actor = actor_tag.text.strip() if actor_tag else \"\"\n    role_tag = li.select_one(\"div.text-ellipsis small\")",
        "detail": "data_scrapper.DramaList_Scrapper.test",
        "documentation": {}
    },
    {
        "label": "ratings[\"content_rating\"]",
        "kind": 5,
        "importPath": "data_scrapper.DramaList_Scrapper.test",
        "description": "data_scrapper.DramaList_Scrapper.test",
        "peekOfCode": "ratings[\"content_rating\"] = get_rating_value(\"Content Rating\")\n# ---------------------------\n# 9. Cast\n# ---------------------------\ncast_list = []\nfor li in soup.select(\"ul.credits li.list-item\"):\n    actor_tag = li.select_one('b[itempropx=\"name\"]')\n    actor = actor_tag.text.strip() if actor_tag else \"\"\n    role_tag = li.select_one(\"div.text-ellipsis small\")\n    role = role_tag.text.strip() if role_tag else \"\"",
        "detail": "data_scrapper.DramaList_Scrapper.test",
        "documentation": {}
    },
    {
        "label": "cast_list",
        "kind": 5,
        "importPath": "data_scrapper.DramaList_Scrapper.test",
        "description": "data_scrapper.DramaList_Scrapper.test",
        "peekOfCode": "cast_list = []\nfor li in soup.select(\"ul.credits li.list-item\"):\n    actor_tag = li.select_one('b[itempropx=\"name\"]')\n    actor = actor_tag.text.strip() if actor_tag else \"\"\n    role_tag = li.select_one(\"div.text-ellipsis small\")\n    role = role_tag.text.strip() if role_tag else \"\"\n    role_type_tag = li.select_one(\"small.text-muted\")\n    role_type = role_type_tag.text.strip() if role_type_tag else \"\"\n    img_tag = li.select_one(\"div.credits-left img\")\n    img_url = img_tag.get(\"data-src\", \"\") if img_tag else \"\"",
        "detail": "data_scrapper.DramaList_Scrapper.test",
        "documentation": {}
    },
    {
        "label": "drama_data",
        "kind": 5,
        "importPath": "data_scrapper.DramaList_Scrapper.test",
        "description": "data_scrapper.DramaList_Scrapper.test",
        "peekOfCode": "drama_data = {\n    \"titles\": titles,\n    \"related_content\": related,\n    \"directors\": directors,\n    \"screenwriters\": screenwriters,\n    \"genres\": genres,\n    \"tags\": tags,\n    \"info\": info,\n    \"ratings\": ratings,\n    \"cast\": cast_list,",
        "detail": "data_scrapper.DramaList_Scrapper.test",
        "documentation": {}
    },
    {
        "label": "output_file",
        "kind": 5,
        "importPath": "data_scrapper.DramaList_Scrapper.test",
        "description": "data_scrapper.DramaList_Scrapper.test",
        "peekOfCode": "output_file = r\"D:\\Projects\\SeoulMate\\data_scrapper\\DramaList_Scrapper\\output.json\"\nwith open(output_file, \"w\", encoding=\"utf-8\") as f:\n    json.dump(drama_data, f, indent=4, ensure_ascii=False)\nprint(f\"Data extracted successfully! Saved to {output_file}\")",
        "detail": "data_scrapper.DramaList_Scrapper.test",
        "documentation": {}
    },
    {
        "label": "extract_drama_data_from_html",
        "kind": 2,
        "importPath": "data_scrapper.html_extractor_and_reader",
        "description": "data_scrapper.html_extractor_and_reader",
        "peekOfCode": "def extract_drama_data_from_html(html_content):\n    \"\"\"\n    Extracts drama information from the given HTML content string.\n    Returns a list of dictionaries.\n    \"\"\"\n    soup = BeautifulSoup(html_content, 'html.parser')\n    drama_items = soup.find_all('div', class_='box')\n    extracted_data = []\n    BASE_URL = \"https://mydramalist.com\"\n    for item in drama_items:",
        "detail": "data_scrapper.html_extractor_and_reader",
        "documentation": {}
    },
    {
        "label": "extract_from_folder",
        "kind": 2,
        "importPath": "data_scrapper.html_extractor_and_reader",
        "description": "data_scrapper.html_extractor_and_reader",
        "peekOfCode": "def extract_from_folder(folder_path, output_csv):\n    \"\"\"\n    Loops through all .html files in the given folder,\n    extracts drama data from each file, and saves everything to one CSV.\n    \"\"\"\n    all_data = []\n    for filename in os.listdir(folder_path):\n        if filename.lower().endswith(\".html\"):\n            file_path = os.path.join(folder_path, filename)\n            print(f\"Processing file: {filename}\")",
        "detail": "data_scrapper.html_extractor_and_reader",
        "documentation": {}
    },
    {
        "label": "URL",
        "kind": 5,
        "importPath": "data_scrapper.kissh_extractor",
        "description": "data_scrapper.kissh_extractor",
        "peekOfCode": "URL = \"https://kisskh.co/Explore?status=2&country=2&sub=1&type=1&order=1\"\n# Use a distinct filename for the debug run\nFILENAME = \"kissh_debug_output.html\"\n# Run the asynchronous function\nif __name__ == \"__main__\":\n    asyncio.run(debug_html_fetch_with_playwright(URL, FILENAME))",
        "detail": "data_scrapper.kissh_extractor",
        "documentation": {}
    },
    {
        "label": "FILENAME",
        "kind": 5,
        "importPath": "data_scrapper.kissh_extractor",
        "description": "data_scrapper.kissh_extractor",
        "peekOfCode": "FILENAME = \"kissh_debug_output.html\"\n# Run the asynchronous function\nif __name__ == \"__main__\":\n    asyncio.run(debug_html_fetch_with_playwright(URL, FILENAME))",
        "detail": "data_scrapper.kissh_extractor",
        "documentation": {}
    },
    {
        "label": "remove_refs",
        "kind": 2,
        "importPath": "data_scrapper.wiki_scrapper_playwright",
        "description": "data_scrapper.wiki_scrapper_playwright",
        "peekOfCode": "def remove_refs(text):\n    \"\"\"Remove all reference markers like [1], [ko], [citation needed].\"\"\"\n    return re.sub(r'\\[.*?\\]', '', str(text))\ndef clean_multiline(text):\n    \"\"\"Convert multiline text to properly comma-separated clean string.\"\"\"\n    if not text or str(text).strip().lower() in [\"n/a\", \"na\"]:\n        return \"-\"\n    text = remove_refs(text)\n    parts = re.split(r'[\\n\\r]+', str(text))\n    parts = [p.strip() for p in parts if p.strip()]",
        "detail": "data_scrapper.wiki_scrapper_playwright",
        "documentation": {}
    },
    {
        "label": "clean_multiline",
        "kind": 2,
        "importPath": "data_scrapper.wiki_scrapper_playwright",
        "description": "data_scrapper.wiki_scrapper_playwright",
        "peekOfCode": "def clean_multiline(text):\n    \"\"\"Convert multiline text to properly comma-separated clean string.\"\"\"\n    if not text or str(text).strip().lower() in [\"n/a\", \"na\"]:\n        return \"-\"\n    text = remove_refs(text)\n    parts = re.split(r'[\\n\\r]+', str(text))\n    parts = [p.strip() for p in parts if p.strip()]\n    return \", \".join(parts) if parts else \"-\"\ndef clean_description(desc):\n    \"\"\"Clean [1], etc. and normalize spaces.\"\"\"",
        "detail": "data_scrapper.wiki_scrapper_playwright",
        "documentation": {}
    },
    {
        "label": "clean_description",
        "kind": 2,
        "importPath": "data_scrapper.wiki_scrapper_playwright",
        "description": "data_scrapper.wiki_scrapper_playwright",
        "peekOfCode": "def clean_description(desc):\n    \"\"\"Clean [1], etc. and normalize spaces.\"\"\"\n    if not desc:\n        return \"-\"\n    desc = remove_refs(desc)\n    desc = re.sub(r'\\s+', ' ', desc).strip()\n    return desc if desc else \"-\"\ndef extract_years_from_release(text):\n    \"\"\"Extract release year(s) from release date text.\"\"\"\n    if not text or str(text).strip().lower() in [\"n/a\", \"na\", \"-\"]:",
        "detail": "data_scrapper.wiki_scrapper_playwright",
        "documentation": {}
    },
    {
        "label": "extract_years_from_release",
        "kind": 2,
        "importPath": "data_scrapper.wiki_scrapper_playwright",
        "description": "data_scrapper.wiki_scrapper_playwright",
        "peekOfCode": "def extract_years_from_release(text):\n    \"\"\"Extract release year(s) from release date text.\"\"\"\n    if not text or str(text).strip().lower() in [\"n/a\", \"na\", \"-\"]:\n        return \"-\"\n    text = remove_refs(text)\n    full_years = re.findall(r'(?:19|20)\\d{2}', text)\n    unique_years = list(dict.fromkeys(full_years))  # preserve order\n    return \", \".join(unique_years) if unique_years else \"-\"\n# =========================================================\n# Fallback: Description method",
        "detail": "data_scrapper.wiki_scrapper_playwright",
        "documentation": {}
    },
    {
        "label": "get_description_fallback",
        "kind": 2,
        "importPath": "data_scrapper.wiki_scrapper_playwright",
        "description": "data_scrapper.wiki_scrapper_playwright",
        "peekOfCode": "def get_description_fallback(url):\n    headers = {\n        \"User-Agent\": (\n            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n            \"Chrome/118.0.0.0 Safari/537.36\"\n        )\n    }\n    try:\n        response = requests.get(url, headers=headers, timeout=10)",
        "detail": "data_scrapper.wiki_scrapper_playwright",
        "documentation": {}
    },
    {
        "label": "remove_refs",
        "kind": 2,
        "importPath": "data_scrapper.wiki_scrapper_selenium",
        "description": "data_scrapper.wiki_scrapper_selenium",
        "peekOfCode": "def remove_refs(text):\n    \"\"\"Remove all reference markers like [1], [ko], [citation needed].\"\"\"\n    return re.sub(r'\\[.*?\\]', '', str(text))\ndef clean_multiline(text):\n    \"\"\"Convert multiline text to properly comma-separated clean string.\"\"\"\n    if not text or str(text).strip().lower() in [\"n/a\", \"na\"]:\n        return \"-\"\n    text = remove_refs(text)\n    parts = re.split(r'[\\n\\r]+', str(text))\n    parts = [p.strip() for p in parts if p.strip()]",
        "detail": "data_scrapper.wiki_scrapper_selenium",
        "documentation": {}
    },
    {
        "label": "clean_multiline",
        "kind": 2,
        "importPath": "data_scrapper.wiki_scrapper_selenium",
        "description": "data_scrapper.wiki_scrapper_selenium",
        "peekOfCode": "def clean_multiline(text):\n    \"\"\"Convert multiline text to properly comma-separated clean string.\"\"\"\n    if not text or str(text).strip().lower() in [\"n/a\", \"na\"]:\n        return \"-\"\n    text = remove_refs(text)\n    parts = re.split(r'[\\n\\r]+', str(text))\n    parts = [p.strip() for p in parts if p.strip()]\n    return \", \".join(parts) if parts else \"-\"\ndef clean_description(desc):\n    \"\"\"Clean [1], etc. and normalize spaces.\"\"\"",
        "detail": "data_scrapper.wiki_scrapper_selenium",
        "documentation": {}
    },
    {
        "label": "clean_description",
        "kind": 2,
        "importPath": "data_scrapper.wiki_scrapper_selenium",
        "description": "data_scrapper.wiki_scrapper_selenium",
        "peekOfCode": "def clean_description(desc):\n    \"\"\"Clean [1], etc. and normalize spaces.\"\"\"\n    if not desc:\n        return \"-\"\n    desc = remove_refs(desc)\n    desc = re.sub(r'\\s+', ' ', desc).strip()\n    return desc if desc else \"-\"\ndef extract_years_from_release(text):\n    \"\"\"Extract release year(s) from release date text.\"\"\"\n    if not text or str(text).strip().lower() in [\"n/a\", \"na\", \"-\"]:",
        "detail": "data_scrapper.wiki_scrapper_selenium",
        "documentation": {}
    },
    {
        "label": "extract_years_from_release",
        "kind": 2,
        "importPath": "data_scrapper.wiki_scrapper_selenium",
        "description": "data_scrapper.wiki_scrapper_selenium",
        "peekOfCode": "def extract_years_from_release(text):\n    \"\"\"Extract release year(s) from release date text.\"\"\"\n    if not text or str(text).strip().lower() in [\"n/a\", \"na\", \"-\"]:\n        return \"-\"\n    text = remove_refs(text)\n    # Match 4-digit years (e.g., 2004, 2015, 2020)\n    years = re.findall(r'(19|20)\\d{2}', text)\n    # Join unique years (sorted in order of appearance)\n    if not years:\n        return \"-\"",
        "detail": "data_scrapper.wiki_scrapper_selenium",
        "documentation": {}
    },
    {
        "label": "get_description_fallback",
        "kind": 2,
        "importPath": "data_scrapper.wiki_scrapper_selenium",
        "description": "data_scrapper.wiki_scrapper_selenium",
        "peekOfCode": "def get_description_fallback(url):\n    headers = {\n        \"User-Agent\": (\n            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n            \"Chrome/118.0.0.0 Safari/537.36\"\n        )\n    }\n    try:\n        response = requests.get(url, headers=headers, timeout=10)",
        "detail": "data_scrapper.wiki_scrapper_selenium",
        "documentation": {}
    },
    {
        "label": "chrome_options",
        "kind": 5,
        "importPath": "data_scrapper.wiki_scrapper_selenium",
        "description": "data_scrapper.wiki_scrapper_selenium",
        "peekOfCode": "chrome_options = Options()\nchrome_options.add_argument(\"--headless\")\nchrome_options.add_argument(\"--no-sandbox\")\nchrome_options.add_argument(\"--disable-dev-shm-usage\")\ndriver = webdriver.Chrome(options=chrome_options)\n# ---------------------------\n# Wikipedia main list\n# ---------------------------\nbase_url = \"https://en.wikipedia.org/wiki/List_of_South_Korean_dramas\"\ndriver.get(base_url)",
        "detail": "data_scrapper.wiki_scrapper_selenium",
        "documentation": {}
    },
    {
        "label": "driver",
        "kind": 5,
        "importPath": "data_scrapper.wiki_scrapper_selenium",
        "description": "data_scrapper.wiki_scrapper_selenium",
        "peekOfCode": "driver = webdriver.Chrome(options=chrome_options)\n# ---------------------------\n# Wikipedia main list\n# ---------------------------\nbase_url = \"https://en.wikipedia.org/wiki/List_of_South_Korean_dramas\"\ndriver.get(base_url)\ntime.sleep(3)\n# ---------------------------\n# Collect all drama links\n# ---------------------------",
        "detail": "data_scrapper.wiki_scrapper_selenium",
        "documentation": {}
    },
    {
        "label": "base_url",
        "kind": 5,
        "importPath": "data_scrapper.wiki_scrapper_selenium",
        "description": "data_scrapper.wiki_scrapper_selenium",
        "peekOfCode": "base_url = \"https://en.wikipedia.org/wiki/List_of_South_Korean_dramas\"\ndriver.get(base_url)\ntime.sleep(3)\n# ---------------------------\n# Collect all drama links\n# ---------------------------\nprint(\"Collecting drama links...\")\ndrama_links = []\nelems = driver.find_elements(By.CLASS_NAME, \"div-col\")\nfor elem in elems:",
        "detail": "data_scrapper.wiki_scrapper_selenium",
        "documentation": {}
    },
    {
        "label": "drama_links",
        "kind": 5,
        "importPath": "data_scrapper.wiki_scrapper_selenium",
        "description": "data_scrapper.wiki_scrapper_selenium",
        "peekOfCode": "drama_links = []\nelems = driver.find_elements(By.CLASS_NAME, \"div-col\")\nfor elem in elems:\n    links = elem.find_elements(By.CSS_SELECTOR, \"a[href]\")\n    for link in links:\n        href = link.get_attribute(\"href\")\n        if href and \"wiki\" in href and \"redlink\" not in href:\n            drama_links.append(href)\ndrama_links = list(set(drama_links))\nprint(f\"Found {len(drama_links)} drama links\")",
        "detail": "data_scrapper.wiki_scrapper_selenium",
        "documentation": {}
    },
    {
        "label": "elems",
        "kind": 5,
        "importPath": "data_scrapper.wiki_scrapper_selenium",
        "description": "data_scrapper.wiki_scrapper_selenium",
        "peekOfCode": "elems = driver.find_elements(By.CLASS_NAME, \"div-col\")\nfor elem in elems:\n    links = elem.find_elements(By.CSS_SELECTOR, \"a[href]\")\n    for link in links:\n        href = link.get_attribute(\"href\")\n        if href and \"wiki\" in href and \"redlink\" not in href:\n            drama_links.append(href)\ndrama_links = list(set(drama_links))\nprint(f\"Found {len(drama_links)} drama links\")\n# ---------------------------",
        "detail": "data_scrapper.wiki_scrapper_selenium",
        "documentation": {}
    },
    {
        "label": "drama_links",
        "kind": 5,
        "importPath": "data_scrapper.wiki_scrapper_selenium",
        "description": "data_scrapper.wiki_scrapper_selenium",
        "peekOfCode": "drama_links = list(set(drama_links))\nprint(f\"Found {len(drama_links)} drama links\")\n# ---------------------------\n# Scrape each drama page\n# ---------------------------\ntitles, alt_titles, writers, directors, casts, genres, networks, episodes, releases, release_years, posters, descriptions = [], [], [], [], [], [], [], [], [], [], [], []\nfor i, url in enumerate(drama_links):  # you can limit with [:10] for testing\n    print(f\"\\nScraping ({i+1}/{len(drama_links)}): {url}\")\n    driver.get(url)\n    time.sleep(1.5)",
        "detail": "data_scrapper.wiki_scrapper_selenium",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "data_scrapper.wiki_scrapper_selenium",
        "description": "data_scrapper.wiki_scrapper_selenium",
        "peekOfCode": "df = pd.DataFrame({\n    \"Title\": titles,\n    \"Also Known As\": alt_titles,\n    \"Written By\": writers,\n    \"Director\": directors,\n    \"Cast\": casts,\n    \"Genre\": genres,\n    \"Network\": networks,\n    \"Episodes\": episodes,\n    \"Release Dates\": releases,",
        "detail": "data_scrapper.wiki_scrapper_selenium",
        "documentation": {}
    },
    {
        "label": "clean_text",
        "kind": 2,
        "importPath": "model_traning.build_index",
        "description": "model_traning.build_index",
        "peekOfCode": "def clean_text(text):\n    return \" \".join(str(text).replace(\"\\n\", \" \").replace(\"\\r\", \" \").split())\nfor col in df.columns:\n    df[col] = df[col].astype(str).apply(clean_text)\n# Create unified text field for embeddings\ntext_features = [\"Title\", \"Genre\", \"Description\", \"Cast\", \"Director\", \"Also Known As\", \"Network\", \"Release Years\"]\ndf[\"content\"] = df[[col for col in text_features if col in df.columns]].agg(\" \".join, axis=1)\n# ======================================================\n# 3. Load SentenceTransformer Model\n# ======================================================",
        "detail": "model_traning.build_index",
        "documentation": {}
    },
    {
        "label": "DATA_PATH",
        "kind": 5,
        "importPath": "model_traning.build_index",
        "description": "model_traning.build_index",
        "peekOfCode": "DATA_PATH = r\"D:\\Projects\\Kdrama-recomendation\\data_scrapper\\kdrama_dataset_detailed_v7.csv\"\nMODEL_NAME = \"paraphrase-multilingual-mpnet-base-v2\"\nMODEL_DIR = r\"D:\\Projects\\Kdrama-recommendation\\model_training\\models\"\nINDEX_DIR = r\"D:\\Projects\\Kdrama-recommendation\\model_training\\faiss_index\"\nos.makedirs(MODEL_DIR, exist_ok=True)\nos.makedirs(INDEX_DIR, exist_ok=True)\n# ======================================================\n# 2. Load and Prepare Dataset\n# ======================================================\nprint(\"Loading dataset...\")",
        "detail": "model_traning.build_index",
        "documentation": {}
    },
    {
        "label": "MODEL_NAME",
        "kind": 5,
        "importPath": "model_traning.build_index",
        "description": "model_traning.build_index",
        "peekOfCode": "MODEL_NAME = \"paraphrase-multilingual-mpnet-base-v2\"\nMODEL_DIR = r\"D:\\Projects\\Kdrama-recommendation\\model_training\\models\"\nINDEX_DIR = r\"D:\\Projects\\Kdrama-recommendation\\model_training\\faiss_index\"\nos.makedirs(MODEL_DIR, exist_ok=True)\nos.makedirs(INDEX_DIR, exist_ok=True)\n# ======================================================\n# 2. Load and Prepare Dataset\n# ======================================================\nprint(\"Loading dataset...\")\ndf = pd.read_csv(DATA_PATH, encoding=\"utf-8\", low_memory=False)",
        "detail": "model_traning.build_index",
        "documentation": {}
    },
    {
        "label": "MODEL_DIR",
        "kind": 5,
        "importPath": "model_traning.build_index",
        "description": "model_traning.build_index",
        "peekOfCode": "MODEL_DIR = r\"D:\\Projects\\Kdrama-recommendation\\model_training\\models\"\nINDEX_DIR = r\"D:\\Projects\\Kdrama-recommendation\\model_training\\faiss_index\"\nos.makedirs(MODEL_DIR, exist_ok=True)\nos.makedirs(INDEX_DIR, exist_ok=True)\n# ======================================================\n# 2. Load and Prepare Dataset\n# ======================================================\nprint(\"Loading dataset...\")\ndf = pd.read_csv(DATA_PATH, encoding=\"utf-8\", low_memory=False)\ndf.fillna(\"\", inplace=True)",
        "detail": "model_traning.build_index",
        "documentation": {}
    },
    {
        "label": "INDEX_DIR",
        "kind": 5,
        "importPath": "model_traning.build_index",
        "description": "model_traning.build_index",
        "peekOfCode": "INDEX_DIR = r\"D:\\Projects\\Kdrama-recommendation\\model_training\\faiss_index\"\nos.makedirs(MODEL_DIR, exist_ok=True)\nos.makedirs(INDEX_DIR, exist_ok=True)\n# ======================================================\n# 2. Load and Prepare Dataset\n# ======================================================\nprint(\"Loading dataset...\")\ndf = pd.read_csv(DATA_PATH, encoding=\"utf-8\", low_memory=False)\ndf.fillna(\"\", inplace=True)\n# Ensure required columns exist",
        "detail": "model_traning.build_index",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "model_traning.build_index",
        "description": "model_traning.build_index",
        "peekOfCode": "df = pd.read_csv(DATA_PATH, encoding=\"utf-8\", low_memory=False)\ndf.fillna(\"\", inplace=True)\n# Ensure required columns exist\nfor col in [\"Title\", \"Genre\", \"Description\", \"Cast\"]:\n    if col not in df.columns:\n        df[col] = \"\"\n# Normalize text fields\ndef clean_text(text):\n    return \" \".join(str(text).replace(\"\\n\", \" \").replace(\"\\r\", \" \").split())\nfor col in df.columns:",
        "detail": "model_traning.build_index",
        "documentation": {}
    },
    {
        "label": "text_features",
        "kind": 5,
        "importPath": "model_traning.build_index",
        "description": "model_traning.build_index",
        "peekOfCode": "text_features = [\"Title\", \"Genre\", \"Description\", \"Cast\", \"Director\", \"Also Known As\", \"Network\", \"Release Years\"]\ndf[\"content\"] = df[[col for col in text_features if col in df.columns]].agg(\" \".join, axis=1)\n# ======================================================\n# 3. Load SentenceTransformer Model\n# ======================================================\nprint(\"Loading SentenceTransformer model...\")\nmodel = SentenceTransformer(MODEL_NAME, cache_folder=MODEL_DIR)\nprint(\"Model loaded successfully!\")\n# ======================================================\n# 4. Generate Embeddings",
        "detail": "model_traning.build_index",
        "documentation": {}
    },
    {
        "label": "df[\"content\"]",
        "kind": 5,
        "importPath": "model_traning.build_index",
        "description": "model_traning.build_index",
        "peekOfCode": "df[\"content\"] = df[[col for col in text_features if col in df.columns]].agg(\" \".join, axis=1)\n# ======================================================\n# 3. Load SentenceTransformer Model\n# ======================================================\nprint(\"Loading SentenceTransformer model...\")\nmodel = SentenceTransformer(MODEL_NAME, cache_folder=MODEL_DIR)\nprint(\"Model loaded successfully!\")\n# ======================================================\n# 4. Generate Embeddings\n# ======================================================",
        "detail": "model_traning.build_index",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "model_traning.build_index",
        "description": "model_traning.build_index",
        "peekOfCode": "model = SentenceTransformer(MODEL_NAME, cache_folder=MODEL_DIR)\nprint(\"Model loaded successfully!\")\n# ======================================================\n# 4. Generate Embeddings\n# ======================================================\nprint(\"Generating embeddings (this may take a few minutes)...\")\nembeddings = model.encode(\n    df[\"content\"].tolist(),\n    show_progress_bar=True,\n    convert_to_numpy=True,",
        "detail": "model_traning.build_index",
        "documentation": {}
    },
    {
        "label": "embeddings",
        "kind": 5,
        "importPath": "model_traning.build_index",
        "description": "model_traning.build_index",
        "peekOfCode": "embeddings = model.encode(\n    df[\"content\"].tolist(),\n    show_progress_bar=True,\n    convert_to_numpy=True,\n    batch_size=32\n)\n# ======================================================\n# 5. Build FAISS Index\n# ======================================================\ndim = embeddings.shape[1]",
        "detail": "model_traning.build_index",
        "documentation": {}
    },
    {
        "label": "dim",
        "kind": 5,
        "importPath": "model_traning.build_index",
        "description": "model_traning.build_index",
        "peekOfCode": "dim = embeddings.shape[1]\nindex = faiss.IndexFlatIP(dim)  # Inner product for cosine similarity\nfaiss.normalize_L2(embeddings)\nindex.add(embeddings)\nprint(f\"FAISS index built successfully with {index.ntotal} items.\")\n# ======================================================\n# 6. Save Index and Metadata\n# ======================================================\nfaiss.write_index(index, os.path.join(INDEX_DIR, \"index.faiss\"))\n# Save relevant metadata (keep it clean for inference)",
        "detail": "model_traning.build_index",
        "documentation": {}
    },
    {
        "label": "index",
        "kind": 5,
        "importPath": "model_traning.build_index",
        "description": "model_traning.build_index",
        "peekOfCode": "index = faiss.IndexFlatIP(dim)  # Inner product for cosine similarity\nfaiss.normalize_L2(embeddings)\nindex.add(embeddings)\nprint(f\"FAISS index built successfully with {index.ntotal} items.\")\n# ======================================================\n# 6. Save Index and Metadata\n# ======================================================\nfaiss.write_index(index, os.path.join(INDEX_DIR, \"index.faiss\"))\n# Save relevant metadata (keep it clean for inference)\nmeta_cols = [\"Title\", \"Genre\", \"Description\", \"Cast\", \"Director\", \"Network\", \"Release Years\", \"Also Known As\"]",
        "detail": "model_traning.build_index",
        "documentation": {}
    },
    {
        "label": "meta_cols",
        "kind": 5,
        "importPath": "model_traning.build_index",
        "description": "model_traning.build_index",
        "peekOfCode": "meta_cols = [\"Title\", \"Genre\", \"Description\", \"Cast\", \"Director\", \"Network\", \"Release Years\", \"Also Known As\"]\nmeta_cols = [c for c in meta_cols if c in df.columns]\nmetadata = df[meta_cols].to_dict(orient=\"records\")\nwith open(os.path.join(INDEX_DIR, \"meta.pkl\"), \"wb\") as f:\n    pickle.dump(metadata, f)\nprint(f\"Index and metadata saved in: {INDEX_DIR}\")\nprint(\"All done! Your FAISS index is ready for recommendations.\")",
        "detail": "model_traning.build_index",
        "documentation": {}
    },
    {
        "label": "meta_cols",
        "kind": 5,
        "importPath": "model_traning.build_index",
        "description": "model_traning.build_index",
        "peekOfCode": "meta_cols = [c for c in meta_cols if c in df.columns]\nmetadata = df[meta_cols].to_dict(orient=\"records\")\nwith open(os.path.join(INDEX_DIR, \"meta.pkl\"), \"wb\") as f:\n    pickle.dump(metadata, f)\nprint(f\"Index and metadata saved in: {INDEX_DIR}\")\nprint(\"All done! Your FAISS index is ready for recommendations.\")",
        "detail": "model_traning.build_index",
        "documentation": {}
    },
    {
        "label": "metadata",
        "kind": 5,
        "importPath": "model_traning.build_index",
        "description": "model_traning.build_index",
        "peekOfCode": "metadata = df[meta_cols].to_dict(orient=\"records\")\nwith open(os.path.join(INDEX_DIR, \"meta.pkl\"), \"wb\") as f:\n    pickle.dump(metadata, f)\nprint(f\"Index and metadata saved in: {INDEX_DIR}\")\nprint(\"All done! Your FAISS index is ready for recommendations.\")",
        "detail": "model_traning.build_index",
        "documentation": {}
    }
]